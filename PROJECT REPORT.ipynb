{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b422fe7",
   "metadata": {},
   "source": [
    "**B√°o c√°o cu·ªëi k·ª≥ m√¥n h·ªçc: PYTHON CHO KHOA H·ªåC D·ªÆ LI·ªÜU**\n",
    "\n",
    "**L·ªõp 23TTH, Khoa To√°n - Tin h·ªçc, Tr∆∞·ªùng ƒê·∫°i h·ªçc Khoa h·ªçc T·ª± nhi√™n, ƒêHQG-HCM**\n",
    "\n",
    "**ƒê·ªÅ t√†i th·ª±c hi·ªán:**\n",
    "$$\n",
    "\\text{\\textbf{USING DEEP LEARNING TO CLASSIFY ANIMAL AND HUMAN IMAGES}}\n",
    "$$\n",
    "\n",
    "**Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n: ThS. H√† VƒÉn Th·∫£o**\n",
    "\n",
    "**Danh s√°ch th√†nh vi√™n nh√≥m:**\n",
    "\n",
    "1. 23110114 - Nguy·ªÖn Th·ªã H·ªìng Th·∫Øm \\\n",
    "2. 23110123 - L√™ Hu·ª≥nh Y·∫øn Vy \\\n",
    "3. 23110132 - Tr·∫ßn Nh·∫≠t Anh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ec77e2",
   "metadata": {},
   "source": [
    "## GI·ªöI THI·ªÜU\n",
    "\n",
    "Object detection l√† m·ªôt trong nh·ªØng ch·ªß ƒë·ªÅ \"n√≥ng\" trong deep learing b·ªüi t√≠nh ·ª©ng d·ª•ng cao trong th·ª±c ti·ªÖn v√† ngu·ªìn d·ªØ li·ªáu d·ªìi d√†o, d·ªÖ chu·∫©n b·ªã. M·ªôt trong nh·ªØng thu·∫≠t to√°n object detection n·ªïi ti·∫øng nh·∫•t l√† **YOLO**.\n",
    "\n",
    "YOLO l√† m√¥ h√¨nh m·∫°ng neuron t√≠ch ch·∫≠p (CNN) ƒë∆∞·ª£c s·ª≠ d·ª•ng ph·ªï bi·ªÉn ƒë·ªÉ nh·∫≠n d·∫°ng c√°c ƒë·ªëi t∆∞·ª£ng trong ·∫£nh ho·∫∑c video. ƒêi·ªÉm ƒë·∫∑c bi·ªát c·ªßa m√¥ h√¨nh n√†y l√† c√≥ kh·∫£ nƒÉng ph√°t hi·ªán t·∫•t c·∫£ c√°c ƒë·ªëi t∆∞·ª£ng trong m·ªôt h√¨nh ·∫£nh ch·ªâ qua m·ªôt l·∫ßn lan truy·ªÅn c·ªßa CNN.\n",
    "\n",
    "C√°c ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng t√°ch bi·ªát b∆∞·ªõc ƒë·ªÅ xu·∫•t v√πng v√† b∆∞·ªõc ph√¢n lo·∫°i, YOLO x·ª≠ l√Ω ƒë·∫ßu v√†o, v·ª´a ph√¢n lo·∫°i ƒë∆∞·ª£c c√°c ƒë·ªëi t∆∞·ª£ng, v·ª´a d·ª± ƒëo√°n ƒë∆∞·ª£c v·ªã tr√≠ c·ªßa ch√∫ng trong m·ªôt l·∫ßn duy nh·∫•t.\n",
    "\n",
    "YOLO c√≥ nghƒ©a l√† \"You only look once\", nghƒ©a l√† ch·ªâ c·∫ßn \"nh√¨n\" m·ªôt l·∫ßn l√† thu·∫≠t to√°n ƒë√£ c√≥ th·ªÉ ph√°t hi·ªán ƒë∆∞·ª£c v·∫≠t th·ªÉ, cho th·∫•y ƒë·ªô nhanh c·ªßa thu·∫≠t to√°n g·∫ßn nh∆∞ l√† real-time.\n",
    "\n",
    "·ª®ng d·ª•ng c·ªßa YOLO c≈©ng nh∆∞ nhi·ªÅu thu·∫≠t to√°n object detection kh√°c, r·∫•t ƒëa d·∫°ng: qu·∫£n l√Ω giao th√¥ng, ƒë·∫øm s·ªë s·∫£n ph·∫©m tr√™n bƒÉng chuy·ªÅn nh√† m√°y, ƒë·∫øm s·ªë v·∫≠t nu√¥i trong chƒÉn nu√¥i, ph√°t hi·ªán v·∫≠t th·ªÉ nguy hi·ªÉm (s√∫ng, dao,...), ch·∫•m c√¥ng t·ª± ƒë·ªông,..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027c9885",
   "metadata": {},
   "source": [
    "## T·∫†O M√îI TR∆Ø·ªúNG ·∫¢O V√Ä KERNEL CH·∫†Y NOTEBOOK (LINUX)\n",
    "\n",
    "D·ª± √°n Python c·∫ßn **m√¥i tr∆∞·ªùng ·∫£o (virtual environment)** ƒë·ªÉ t·ª± c√°ch ly, tr√°nh xung ƒë·ªôt phi√™n b·∫£n th∆∞ vi·ªán gi·ªØa c√°c d·ª± √°n. `venv` l√† m√¥i tr∆∞·ªùng ·∫£o m√† ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng trong d·ª± √°n n√†y. Sau khi c√†i ƒë·∫∑t `venv`, ch√∫ng ta di chuy·ªÉn ƒë∆∞·ªùng d·∫´n ƒë·∫øn folder ch·ª©a d·ª± √°n trong terminal v√† s·ª≠ d·ª•ng l·ªánh sau ƒë·ªÉ c√†i ƒë·∫∑t m√¥i tr∆∞·ªùng ·∫£o cho d·ª± √°n:\n",
    "\n",
    "`python -m venv .venv`\n",
    "\n",
    "Trong ƒë√≥, `.venv` l√† t√™n c·ªßa folder ch·ª©a m√¥i tr∆∞·ªùng ·∫£o c·ªßa d·ª± √°n, ƒë·ªìng th·ªùi n√≥ c≈©ng s·∫Ω \"ƒë√≥ng bƒÉng\" phi√™n b·∫£n Python, pip v√† c√°c th∆∞ vi·ªán s·∫Ω ƒë∆∞·ª£c d√πng trong d·ª± √°n.\n",
    "\n",
    "K√≠ch ho·∫°t m√¥i tr∆∞·ªùng ·∫£o:\n",
    "\n",
    "`source .venv/bin/activate`\n",
    "\n",
    "L√∫c n√†y, phi√™n b·∫£n Python v√† `pip` ƒë∆∞·ª£c d√πng l√† c·ªßa m√¥i tr∆∞·ªùng ·∫£o, c√°c th∆∞ vi·ªán c√†i b·∫±ng `pip install` c≈©ng ch·ªâ ·∫£nh h∆∞·ªüng trong `.venv`. C√°ch nh·∫≠n bi·∫øt ƒëang ·ªü m√¥i tr∆∞·ªùng ·∫£o l√† promt terminal th∆∞·ªùng ƒë·ªïi th√†nh `(.venv) user_name@machine:~` (n·∫øu ƒëang s·ª≠ d·ª•ng Linux). Khi ƒë√£ k√≠ch ho·∫°t m√¥i tr∆∞·ªùng ·∫£o, ƒë·∫£m b·∫£o phi√™n b·∫£n Python v√† `pip` ƒë√£ \"ƒë√≥ng bƒÉng\" trong ƒë√≥, s·ª≠ d·ª•ng l·ªánh:\n",
    "\n",
    "`which python && which pip`\n",
    "\n",
    "N·∫øu output c√≥ d·∫°ng `.../<project_name>/.venv/...` th√¨ m√¥i tr∆∞·ªùng ·∫£o ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t th√†nh c√¥ng.\n",
    "\n",
    "Ti·∫øp theo, t·∫°o m·ªôt kernel ƒë·ªÉ ch·∫°y Jupyter Notebook. C√†i ƒë·∫∑t `ipykernel` ƒë·ªÉ t·∫°o kernel:\n",
    "\n",
    "`python -m pip install ipykernel`\n",
    "\n",
    "Sau khi c√†i ƒë·∫∑t th√†nh c√¥ng, ti·∫øn h√†nh t·∫°o kernel ƒë·ªÉ ch·∫°y file `.ipynb`:\n",
    "\n",
    "`python -m ipykernel install --prefix .venv --name yolovenv --display-name \"this_project\"`\n",
    "\n",
    "`--prefix .venv`: kernel m·∫∑c ƒë·ªãnh kh√¥ng t·ª± l∆∞u v√†o `.venv`, thu·ªôc t√≠nh n√†y s·∫Ω l∆∞u kernel ƒë√£ t·∫°o v√†o `.venv`  \n",
    "`--name yolovenv`: t√™n folder ch·ª©a kernel, ·ªü ƒë√¢y t√™n folder l√† `yolovenv`. Kernel s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i `.venv/share/jupyter/kernels/yolovenv/`  \n",
    "`--display-name \"this_project`: kernel s·∫Ω hi·ªÉn th·ªã d∆∞·ªõi t√™n `this_project` trong VS Code.\n",
    "\n",
    "Khi ƒë√£ t·∫°o kernel, click v√†o bi·ªÉu t∆∞·ª£ng kernel ·ªü g√≥c tr√™n b√™n ph·∫£i, ch·ªçn\n",
    "$$\n",
    "\\text{Select Another Kernel} \\rightarrow \\text{Jupyter Kernel...} \\rightarrow \\text{this\\_project}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f1757",
   "metadata": {},
   "source": [
    "## KHAI B√ÅO TH∆Ø VI·ªÜN V√Ä CHU·∫®N B·ªä D·ªÆ LI·ªÜU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513c7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c99e5",
   "metadata": {},
   "source": [
    "Data ƒë∆∞·ª£c t·∫£i v·ªÅ t·∫°i c√°c ngu·ªìn sau: \\\n",
    "- https://www.kaggle.com/datasets/antoreepjana/animals-detection-images-dataset \\\n",
    "- https://www.kaggle.com/datasets/biancaferreira/african-wildlife \\\n",
    "- https://www.kaggle.com/datasets/wutheringwang/dog-face-detectionyolo-format \\\n",
    "- https://www.kaggle.com/datasets/samuelayman/cat-dataset\n",
    "- https://universe.roboflow.com/labo-yolo/age-and-gender-xlnfj/dataset/3 \\\n",
    "\n",
    "Data sau khi ƒë∆∞·ª£c t·∫£i v·ªÅ s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω (g√°n l·∫°i class ID; ph√¢n lo·∫°i th√†nh c√°c folder train, valid, test;...), sau ƒë√≥ ƒë∆∞·ª£c g·ªôp th√†nh m·ªôt folder dataset duy nh·∫•t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3253f",
   "metadata": {},
   "source": [
    "Sau khi ho√†n t·∫•t x·ª≠ l√Ω dataset, ch√∫ng ta s·∫Ω ki·ªÉm tra dataset c√≥ b·ªã thi·∫øu **nh√£n d·ªØ li·ªáu** ·ª©ng v·ªõi m·ªói ·∫£nh hay kh√¥ng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96bf0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset: dataset/completed_dataset\n",
      "\n",
      "       total_images  missing_labels  missing_labels (%)\n",
      "train       20451.0             0.0                 0.0\n",
      "valid        6078.0             0.0                 0.0\n",
      "test         4637.0             0.0                 0.0\n",
      "\n",
      "\n",
      "Total images: \t 31166\n",
      "Missing: \t 0\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path(\"dataset/completed_dataset\")\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\"}\n",
    "SPLITS = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "\n",
    "def count_missing(split):\n",
    "    images_dir = BASE_DIR / \"images\" / split\n",
    "    labels_dir = BASE_DIR / \"labels\" / split\n",
    "\n",
    "    total_images = 0\n",
    "    missing = 0\n",
    "\n",
    "    for img in images_dir.iterdir():\n",
    "        if img.suffix.lower() not in IMAGE_EXTS:\n",
    "            continue\n",
    "\n",
    "        total_images += 1\n",
    "        label_path = labels_dir / f\"{img.stem}.txt\"\n",
    "        if not label_path.exists():\n",
    "            missing += 1\n",
    "\n",
    "    return total_images, missing\n",
    "\n",
    "\n",
    "print(f\"Checking dataset: {str(BASE_DIR)}\\n\")\n",
    "\n",
    "grand_total = 0\n",
    "grand_missing = 0\n",
    "total_images_arr = np.array([])\n",
    "missing_arr = np.array([])\n",
    "percent_arr = np.array([])\n",
    "\n",
    "for split in SPLITS:\n",
    "    total_images, missing = count_missing(split)\n",
    "    grand_total += total_images\n",
    "    grand_missing += missing\n",
    "    percent = (missing / total_images * 100) if total_images > 0 else 0\n",
    "\n",
    "    total_images_arr = np.append(total_images_arr, total_images)\n",
    "    missing_arr = np.append(missing_arr, missing)\n",
    "    percent_arr = np.append(percent_arr, percent)\n",
    "\n",
    "np_table = np.array([total_images_arr, missing_arr, percent_arr])\n",
    "table = pd.DataFrame(np_table).transpose()\n",
    "table.columns = [\"total_images\", \"missing_labels\", \"missing_labels (%)\"]\n",
    "table.index = [\"train\", \"valid\", \"test\"]\n",
    "print(table)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Total images: \\t {grand_total}\")\n",
    "print(f\"Missing: \\t {grand_missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da97b8",
   "metadata": {},
   "source": [
    "Theo k·∫øt qu·∫£ ƒë∆∞·ª£c in ra, dataset c√≥ t·ªïng c·ªông 31166 h√¨nh ·∫£nh v√† kh√¥ng c√≥ ·∫£nh n√†o kh√¥ng c√≥ label. Nh∆∞ v·∫≠y, data ƒë√£ ƒë√∫ng v·ªõi format c·ªßa YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54685499",
   "metadata": {},
   "source": [
    "## KHAI B√ÅO, HU·∫§N LUY·ªÜN V√Ä L∆ØU M√î H√åNH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873fb78b",
   "metadata": {},
   "source": [
    "H√†m train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25bea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset_path, project_path, project_name):\n",
    "    model.train(\n",
    "        data=dataset_path,\n",
    "        epochs=50,  # N·∫øu ·ªïn -> 50-80\n",
    "        imgsz=416,  # N·∫øu ·ªïn -> 640\n",
    "        batch=8,  # N·∫øu ·ªïn -> 8\n",
    "        workers=4, # N·∫øu ·ªïn -> 4\n",
    "        device=0,  # S·ª≠ d·ª•ng GPU\n",
    "        project=project_path,\n",
    "        name=project_name,\n",
    "\n",
    "        # Augmentation\n",
    "        mosaic=0.25,\n",
    "        mixup=0.0,\n",
    "        copy_paste=0.0,\n",
    "\n",
    "        # Optimization\n",
    "        lr0=0.005,\n",
    "        optimizer=\"SGD\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc73cc",
   "metadata": {},
   "source": [
    "S·ª≠ d·ª•ng model YOLO11s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1227b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"yolo11s.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4cb71",
   "metadata": {},
   "source": [
    "Hu·∫•n luy·ªán."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e345bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model hasn't been trained. Start training...\n",
      "New https://pypi.org/project/ultralytics/8.3.246 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.243 üöÄ Python-3.14.2 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce GTX 1650, 3716MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=dataset/completed_dataset/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=416, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=0.25, multi_scale=False, name=my_model3, nbs=64, nms=False, opset=None, optimize=False, optimizer=SGD, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/yolo11s_custom, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/runs/yolo11s_custom/my_model3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=85\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    852303  ultralytics.nn.modules.head.Detect           [85, [128, 256, 512]]         \n",
      "YOLO11s summary: 181 layers, 9,460,687 parameters, 9,460,671 gradients, 21.7 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mAMP: \u001b[0mchecks failed ‚ùå. AMP training on NVIDIA GeForce GTX 1650 GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.4¬±0.2 ms, read: 228.0¬±115.2 MB/s, size: 192.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/dataset/completed_dataset/labels/train... 20451 images, 1465 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20451/20451 2.2Kit/s 9.2s<0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/dataset/completed_dataset/images/train/KUZnVA2BbDsY.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/dataset/completed_dataset/images/train/qNvOTY2pa3aM.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/dataset/completed_dataset/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 1.8¬±1.2 ms, read: 176.8¬±137.4 MB/s, size: 341.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/dataset/completed_dataset/labels/valid... 6078 images, 481 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6078/6078 2.0Kit/s 3.1s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0m/home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/dataset/completed_dataset/images/valid/H3NaznckKtOn.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/dataset/completed_dataset/labels/valid.cache\n",
      "Plotting labels to /home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/runs/yolo11s_custom/my_model3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.005, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/buddyy/coding_corner/python_for_data_science/project/classify-animal-and-human-images/runs/yolo11s_custom/my_model3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50      2.94G       1.05      5.291       1.37          9        416: 13% ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 331/2557 3.7it/s 1:46<9:550\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Train model d·ª±a tr√™n model g·ªëc l√† YOLO11s\u001b[39;00m\n\u001b[32m     16\u001b[39m model = YOLO(BASE_MODEL)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load l·∫°i best.pt sau khi train, n·∫øu kh√¥ng t√¨m th·∫•y th√¨ in ra l·ªói\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m MODEL_PATH.exists(), \u001b[33m\"\u001b[39m\u001b[33mTraining finished but file best.pt not found\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataset_path, project_path, project_name)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model\u001b[39m(model, dataset_path, project_path, project_name):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# N·∫øu ·ªïn -> 50-80\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m416\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# N·∫øu ·ªïn -> 640\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# N·∫øu ·ªïn -> 8\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# N·∫øu ·ªïn -> 4\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# S·ª≠ d·ª•ng GPU\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Augmentation\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmosaic\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmixup\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy_paste\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Optimization\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSGD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding_corner/python_for_data_science/project/classify-animal-and-human-images/.venv/lib64/python3.14/site-packages/ultralytics/engine/model.py:773\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    770\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    771\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding_corner/python_for_data_science/project/classify-animal-and-human-images/.venv/lib64/python3.14/site-packages/ultralytics/engine/trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding_corner/python_for_data_science/project/classify-animal-and-human-images/.venv/lib64/python3.14/site-packages/ultralytics/engine/trainer.py:436\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(\u001b[38;5;28mself\u001b[39m.loss).backward()\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     last_opt_step = ni\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding_corner/python_for_data_science/project/classify-animal-and-human-images/.venv/lib64/python3.14/site-packages/ultralytics/engine/trainer.py:687\u001b[39m, in \u001b[36mBaseTrainer.optimizer_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ema:\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/coding_corner/python_for_data_science/project/classify-animal-and-human-images/.venv/lib64/python3.14/site-packages/ultralytics/utils/torch_utils.py:655\u001b[39m, in \u001b[36mModelEMA.update\u001b[39m\u001b[34m(self, model)\u001b[39m\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m v.dtype.is_floating_point:  \u001b[38;5;66;03m# true for FP16 and FP32\u001b[39;00m\n\u001b[32m    654\u001b[39m     v *= d\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     v += (\u001b[32m1\u001b[39m - d) * msd[k].detach()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"dataset/completed_dataset/data.yaml\"\n",
    "\n",
    "# L∆∞u model t·∫°i \"runs/yolo11s_custom/my_model\" sau khi train\n",
    "MODEL_PATH = Path(\"runs/yolo11s_custom/my_model3/weights/best.pt\")\n",
    "PROJECT_PATH = \"runs/yolo11s_custom\"\n",
    "PROJECT_NAME = \"my_model3\"\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi dataset\n",
    "if MODEL_PATH.exists():\n",
    "    print(f\"Model has been trained already. It is being loaded again: {MODEL_PATH}\")\n",
    "    model = YOLO(str(MODEL_PATH))\n",
    "else:\n",
    "    print(\"Model hasn't been trained. Start training...\")\n",
    "\n",
    "    # Train model d·ª±a tr√™n model g·ªëc l√† YOLO11s\n",
    "    model = YOLO(BASE_MODEL)\n",
    "    train_model(model, DATASET_PATH, PROJECT_PATH, PROJECT_NAME)\n",
    "\n",
    "    # Load l·∫°i best.pt sau khi train, n·∫øu kh√¥ng t√¨m th·∫•y th√¨ in ra l·ªói\n",
    "    assert MODEL_PATH.exists(), \"Training finished but file best.pt not found\"\n",
    "    model = YOLO(str(MODEL_PATH))\n",
    "\n",
    "    print(\"Training finished\")\n",
    "\n",
    "# Ki·ªÉm tra v√† kho√° model v·ªõi model g·ªëc l√† YOLO11s\n",
    "# assert model.model.yaml['name'] == 'yolo11s'\n",
    "# model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb95f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2eaa45e",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "this_project",
   "language": "python",
   "name": "yolovenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
